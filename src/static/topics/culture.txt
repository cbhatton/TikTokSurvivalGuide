Fighting the Algorithm: Navigating TikTok’s Culture Bubble

In order to figure out how to fight the TikTok algorithm there are certain terms we must outline and understand first. The following is a list of terms that have been defined that will be used throughout the rest of the page.
Algorithm: “a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer.” (Oxford Languages)
For You Page (FYP): “an individual landing page for users which showcases curated videos that TikTok thinks they might watch or like.” (dictionary.com)
Subculture: “a cultural group within a larger culture, often having beliefs or variance with those of the larger culture.” (Oxford Languages)
In terms of TikTok we can see that the algorithm is a set of rules followed to get videos that the user will enjoy on their for you page. TikTok has said that the factors that contribute to creating the For You Page for each user are user interactions, video information, and device and account settings. User interactions are things such as videos that the user likes, comments they post, videos they share, content that they create and accounts that they follow. Some other aspects of user interactions that were completions and re-watches. An article on Wired.com states that “…TikTok has determined they may be more likely to engage with the video, based on their past behavior. If they respond favorably—say, by sharing the video or watching it in full—TikTok then shows it to more people who it thinks share similar interests.” This means that TikTok pays attention to if a video catches your attention and then how long the video holds your attention. If the video holds your attention for the entirety of the video, then it is seen as an entertaining video that you and potentially many others favor. Then if the video is re-watched it’s rating of enjoyability is boosted even higher. Video information includes sounds that are used on videos, hashtags used on posts, and captions on posts. Device and account settings would be things such as your country, type of device, and language choice.
In the Wall Street Journal video “How TikTok’s Algorithm Figures You Out” they conduct an experiment to check the algorithm. They do this by creating over 100 bots that have imbedded interests that are not disclosed to TikTok. In the video they state “Officially, the company says that shares, likes, follows and what you watch all play a role in what TikTok shows you. We found that TikTok only needs one of these to figure you out.” While all these aspects of user interactions may go into the production of the user for you page there is one that is most depended on to create a perfect page for the user. That is “How long you linger over a piece of content. Every second you hesitate or re-watch, the app is tracking you.”
@https://youtu.be/nfczi2cI6Cs
With the knowledge of all that the algorithm knows about you and what it does with that information now the question we need to ask is if we think this is okay. In the article Democratizing Algorithmic Fairness, it is said that “Algorithms can inherit questionable values from datasets and acquire biases in the course of (machine) learning.” So, let’s define what ways that your TikTok for you page can inherit some biases that effect your scrolling experience. One of the most known bias is the racial bias on TikTok. In the article What Internet Outrage Reveals About Race and TikTok’s Algorithm it is said that “On TikTok, there’s a flipside to the white soties that get the most mainstream attention – Black users have consistently had to fight for visibility and credit. Last year, creators flagged that terms like ‘Black Lives Matter’ and ‘Black people’ were seemingly being suppressed by automated moderation. The Black dancers and choreographers who consistently created the biggest dance trends on the Internet watched white users skyrocket to popularity by copying their work – to the point of a content strike, also last year.” Here we can see that there is a very evident issue with the fairness of the algorithm.
So, what would algorithmic fairness look like and how would we successfully acquire such fairness? Another section of the article states that “…there are more to creating algorithmic fairness than a set of technical tasks, i.e., there is an important political dimension in the problem of algorithmic fairness due to the contentious nature of the ideas of ‘fairness’ and the fact that a decision on fairness measure and the balance between fairness and performance is in effect about competing values.” Although there is no real answer to the problem of algorithmic biases, we can conclude that many different factors need to be thought of while creating an algorithm due to the multiple aspects of why the algorithm will have biases.